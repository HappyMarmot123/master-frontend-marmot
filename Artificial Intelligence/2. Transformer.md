## 1. 트랜스포머(Transformer) 개요 및 병렬 처리

트랜스포머는 기계 번역 분야에서 혁신적인 성능 향상을 이끌어낸 딥러닝 아키텍처이며, GPT, BERT 등 현대 LLM의 기반이 되었다. 트랜스포머는 모든 단어를 동시에 처리한다. 문장 전체를 한 번에 입력받아 각 단어 간의 관계를 병렬로 계산한다. 이것이 가능한 이유는 어텐션(Attention) 메커니즘 덕분이다.

트랜스포머 이전에는 RNN(Recurrent Neural Network)과 LSTM(Long Short-Term Memory)이 자연어 처리의 주류였다. 이 모델들은 순차적으로 단어를 처리하는 구조를 가졌다. RNN/LSTM의 한계는 크게 두 가지였다. 순차 처리로 인한 속도 저하로 인해 앞 단어의 처리가 끝나야 다음 단어를 처리할 수 있어 병렬화가 어려웠다. 그리고 문장이 길어지면 앞부분의 정보가 뒷부분까지 전달되기 어려운 장거리 의존성 문제가 있었다.

### GPU 활용과 학습 효율성

트랜스포머의 병렬 처리 특성은 GPU의 강점과 잘 맞는다. GPU는 수천 개의 코어를 가지고 있어 동일한 연산을 동시에 수행하는 데 최적화되어 있다. 트랜스포머의 어텐션 연산은 행렬 곱셈으로 표현되며, 이는 GPU에서 매우 효율적으로 처리된다.

트랜스포머는 높은 성능을 제공하지만, 그만큼 **많은 연산량을 필요**로 한다. 특히 어텐션 연산은 입력 길이의 제곱에 비례하는 복잡도를 가진다. 1,000개의 토큰을 처리할 때보다 2,000개의 토큰을 처리할 때 4배의 연산이 필요하다.

## 2. 어텐션 메커니즘 (Attention Mechanism)

어텐션은 문장을 이해할 때 모든 단어에 동일한 주의를 기울이지 않고, 문맥상 중요한 단어에 더 집중하는 것과 같다. 

Query, Key, Value 세 가지 개념으로 작동한다. Query와 Key의 유사도를 계산하여 어떤 단어에 얼마나 주목할지 결정하고, 그 가중치에 따라 Value를 조합하여 최종 출력을 만든다.

**Query(질의)**: "무엇을 찾고 있는가?" 현재 처리 중인 단어가 다른 단어들에게 던지는 질문이다.  
**Key(키)**: "나는 어떤 정보를 가지고 있는가?" 각 단어가 자신을 설명하는 정보다.  
**Value(값)**: "실제로 전달할 내용은 무엇인가?" 각 단어가 가진 실제 의미 정보다.  

Self-Attention(자기 어텐션)은 같은 문장 내 단어들 간의 관계를 파악한다. "그 고양이가 매트 위에 앉았다. 그것은 편안해 보였다"라는 문장에서 "그것"이 "고양이"를 가리킨다는 것을 이해하려면, 문장 내 다른 단어들과의 관계를 분석해야 한다. 

"그것"이라는 단어의 Query는 문장 내 모든 단어의 Key와 비교된다. "고양이"의 Key와 높은 유사도를 보이면, "고양이"의 Value가 "그것"의 표현에 강하게 반영된다. 이 과정이 모든 단어 쌍에 대해 동시에 일어난다.

### Multi-Head Attention

Multi-Head Attention은 **여러 개의 어텐션을 병렬로 수행**하는 기법이다. 하나의 어텐션만으로는 단어 간의 다양한 관계를 모두 포착하기 어렵다. 문법적 관계, 의미적 관계, 지시 관계 등 여러 유형의 관계가 존재하기 때문이다.

예를 들어 8개의 어텐션 헤드를 사용한다면, 각 헤드가 서로 다른 관점에서 단어 간 관계를 분석한다. 첫 번째 헤드는 주어-동사 관계에, 두 번째 헤드는 형용사-명사 관계에 집중할 수 있다. 각 헤드의 결과를 합쳐 최종 표현을 만든다.

### 맥락을 모두 참조하는 방식

트랜스포머가 RNN보다 뛰어난 핵심 이유는 **모든 맥락을 직접 참조**할 수 있다는 점이다. RNN에서 문장 앞부분의 정보가 뒷부분에 전달되려면 여러 단계를 거쳐야 했다. 정보가 전달되는 과정에서 손실이 발생하고, 거리가 멀수록 관계 파악이 어려웠다.

트랜스포머의 어텐션은 거리에 상관없이 모든 단어 쌍의 관계를 직접 계산한다. 문장의 첫 단어와 마지막 단어도 한 번의 어텐션 연산으로 연결된다. 이것이 트랜스포머가 장거리 의존성 문제를 해결하고, 복잡한 문맥을 이해할 수 있는 이유다.

## 3. 트랜스포머 기반 주요 모델

트랜스포머 아키텍처를 활용한 대표 모델 3종(BERT, GPT, T5)의 주요 특징과 장단점을 아래 표로 정리하였다.

| 모델  | 구조 | 학습 방식 | 대표 특징 | 장점 | 한계 |
|-------|------|-----------|----------|------|------|
| **BERT**<br>(Bidirectional Encoder Representations from Transformers) | 인코더(Encoder)만 사용<br>(양방향 구조) | 마스크 언어 모델(Masked Language Model, MLM)<br>- 일부 단어를 [MASK]로 가려 예측 | - 입력 문장 전체의 양방향 문맥 활용<br>- 문맥 이해에 강점 | - 문맥 이해↑ (감정 분석, 질의응답 등)<br>- 작은 모델로도 좋은 성능<br>- 미세조정(Fine-tuning)이 효과적 | - 텍스트 생성에는 부적합 (양방향 특성)<br>- 입력 길이 제한(보통 512 토큰) |
| **GPT**<br>(Generative Pre-trained Transformer) | 디코더(Decoder)만 사용<br>(단방향/자기회귀 구조) | 다음 단어 예측(Next Token Prediction)<br>- 이전 단어들만 보고 다음 단어 생성 | - 텍스트 생성에 최적화<br>- 단방향(이전 토큰만 참조) | - 텍스트 생성 능력 탁월<br>- 모델이 클수록 다양한 능력 창발<br>- 프롬프트로 다양한 작업 가능 | - 문맥 이해(NLU)에 상대적 약점<br>- 추론 비용이 높음(대형 모델)<br>- 환각(Hallucination) 발생 가능 |
| **T5**<br>(Text-to-Text Transfer Transformer) | 인코더-디코더 전체 사용 | Text-to-Text 프레임워크<br>- 모든 작업을 텍스트→텍스트로 변환 | - 번역·요약 등 입력-출력 다른 작업에 강점<br>- 다양한 작업을 하나의 틀로 처리 | - 다양한 작업을 통일된 프레임워크에서 처리<br>- 작업 간 지식 전이 효과적 | - 모델이 크고 복잡<br>- 단순 분류에는 과도할 수 있음 |

**요약:**  
- BERT는 "문맥 이해"에, GPT는 "텍스트 생성"에, T5는 "입력-출력 구조가 다른 다양한 NLP 작업 처리"에 각각 최적화되어 있다.
