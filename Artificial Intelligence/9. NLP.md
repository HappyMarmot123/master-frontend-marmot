# NLP (Natural Language Processing, 자연어 처리)

자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있도록 하는 인공지능의 한 분야입니다. 우리가 일상에서 사용하는 번역 서비스, 챗봇, 음성 인식, 검색 엔진 등은 모두 NLP 기술을 기반으로 합니다.

일상적인 비유로 설명하자면, NLP는 "다국어를 구사하는 통역사"와 같습니다. 통역사가 여러 언어를 이해하고 번역하듯이, NLP 시스템은 텍스트를 분석하고, 의미를 파악하며, 다른 형태로 변환할 수 있습니다. 하지만 통역사와 달리, NLP는 수백만 개의 문서를 동시에 학습하고, 패턴을 찾아내며, 일관된 방식으로 처리할 수 있습니다.

### 자연어의 특성

인간의 언어는 매우 복잡합니다. 같은 단어라도 문맥에 따라 의미가 달라지고, 어순이 바뀌어도 의미를 전달할 수 있으며, 비유나 은유 같은 추상적인 표현도 사용합니다. 이러한 자연어의 특성 때문에 컴퓨터가 언어를 이해하고 처리하는 것은 매우 어려운 문제입니다.

예를 들어, "배가 아프다"라는 문장에서 "배"는 과일을 의미할 수도 있고, 배꼽을 의미할 수도 있습니다. 하지만 문맥을 보면 대부분 올바르게 이해할 수 있습니다. NLP는 이러한 문맥을 이해하고 올바른 의미를 파악하는 것을 목표로 합니다.

### 전처리의 중요성

NLP에서 텍스트를 처리하기 전에 먼저 전처리(Preprocessing)를 수행합니다. 이는 원시 텍스트를 컴퓨터가 처리하기 쉬운 형태로 변환하는 과정입니다.

1. **토큰화(Tokenization)**: 문장을 단어나 더 작은 단위로 나누기
2. **정규화(Normalization)**: 대소문자 통일, 특수문자 제거 등
3. **불용어 제거(Stop Word Removal)**: "은", "는", "이", "가" 같은 의미가 적은 단어 제거
4. **어간 추출(Stemming)**: 단어의 어간만 추출하여 변형된 형태를 통일
5. **품사 태깅(POS Tagging)**: 각 단어의 품사(명사, 동사, 형용사 등)를 태그

### 전통적인 표현 방식의 한계

전통적으로 NLP에서는 단어를 숫자로 표현하기 위해 원-핫 인코딩(One-Hot Encoding)을 사용했습니다. 이 방식은 각 단어를 고유한 숫자로 표현하지만, 단어 간의 관계나 유사성을 전혀 표현하지 못합니다.

예를 들어, "고양이"와 "강아지"는 모두 동물이라는 공통점이 있지만, 원-핫 인코딩에서는 완전히 다른 숫자로 표현되어 서로 전혀 관련이 없는 것처럼 보입니다. 또한 어휘가 많아질수록 벡터의 차원이 매우 커져서 계산이 비효율적이 됩니다.

## Word2Vec: 단어의 의미를 벡터로 표현

Word2Vec은 2013년 Google에서 제안한 혁신적인 방법으로, 단어의 의미를 벡터로 표현할 수 있게 해주었습니다. 핵심 아이디어는 "비슷한 맥락에서 나타나는 단어들은 비슷한 의미를 가진다"는 것입니다.

Word2Vec은 대량의 텍스트를 학습하여, 각 단어를 수백 차원의 벡터로 표현합니다. 이 벡터의 마법은 의미적으로 유사한 단어들이 벡터 공간에서 가까운 위치에 배치된다는 것입니다.

예를 들어:

- "고양이"와 "강아지"는 벡터 공간에서 가까이 있습니다
- "서울"과 "부산"도 가까이 있습니다
- "고양이"와 "서울"은 멀리 떨어져 있습니다

또한 Word2Vec은 단어 간의 관계도 학습합니다. 예를 들어, "왕" 벡터에서 "남자" 벡터를 빼고 "여자" 벡터를 더하면 "여왕" 벡터에 가까워집니다. 이는 Word2Vec이 단어의 의미뿐만 아니라 단어 간의 관계도 학습했다는 것을 보여줍니다.

**Word2Vec의 한계**:

Word2Vec은 단어의 의미를 잘 표현하지만, 한 가지 중요한 한계가 있습니다. 같은 단어라도 문맥에 따라 의미가 달라질 수 있는데, Word2Vec은 단어당 하나의 벡터만 가지므로 문맥을 고려하지 못합니다.

예를 들어, "은행"이라는 단어는 금융 기관을 의미할 수도 있고, 강가를 의미할 수도 있습니다. 하지만 Word2Vec은 "은행"을 하나의 벡터로만 표현하므로, 문맥에 따른 의미 차이를 반영하지 못합니다.

### BERT: 문맥을 고려한 표현

BERT(Bidirectional Encoder Representations from Transformers)는 2018년 Google에서 제안한 모델로, Word2Vec의 한계를 극복했습니다. BERT의 핵심 혁신은 **"같은 단어라도 문맥에 따라 다른 벡터를 생성한다"**는 것입니다.

**BERT의 작동 원리**:

BERT는 Transformer의 인코더 구조를 사용하여, 문장의 모든 단어를 동시에 고려합니다. 즉, 각 단어의 의미를 결정할 때 문장의 다른 모든 단어를 참고합니다.

예를 들어, "은행에 가서 돈을 찾았다"라는 문장에서 BERT는 "은행"이라는 단어를 처리할 때, "돈", "찾았다" 같은 단어들을 함께 고려하여 이 "은행"이 금융 기관을 의미한다는 것을 이해합니다. 반면, "강가의 은행에 앉았다"라는 문장에서는 "강가", "앉았다"를 고려하여 이 "은행"이 강가를 의미한다는 것을 이해합니다.

BERT의 이름에 "Bidirectional"이 포함된 이유는, 단어를 이해할 때 앞뒤 문맥을 모두 고려하기 때문입니다. 기존의 모델들은 문장을 한 방향으로만 읽었지만, BERT는 양방향으로 읽어서 더 정확한 의미 파악이 가능합니다.

## NMT (Neural Machine Translation, 신경망 기계 번역)

딥러닝(Deep Learning) 기술을 사용하여 한 언어의 문장을 다른 언어의 문장으로 번역하는 기술입니다. 기존의 규칙 기반이나 통계 기반 기계 번역의 한계를 극복하고, 현재 구글 번역, 파파고 등 대부분의 상용 번역 서비스의 핵심 기술로 사용됩니다.

NMT의 핵심 구조: 인코더-디코더
NMT는 주로 인코더-디코더(Encoder-Decoder) 구조를 기반으로 합니다.

인코더 (Encoder): 입력 문장(예: 한국어) 전체를 읽고, 그 문장의 **의미(문맥 벡터)**를 하나의 압축된 형태로 변환합니다.

디코더 (Decoder): 인코더가 생성한 문맥 벡터를 받아서, 목표 언어(예: 영어)의 문장으로 단어 하나씩 순차적으로 생성합니다.

문장 전체의 의미를 파악하여 번역하므로, 단어 단위 번역보다 훨씬 자연스럽고 유창한 번역 결과를 제공합니다. 유연성 언어 간의 복잡한 규칙을 직접 코딩할 필요 없이, 대량의 병렬 데이터(번역 쌍 데이터)를 학습하여 스스로 패턴을 익힙니다. 특히 Transformer 기반 모델들은 긴 문장의 의존성 문제를 해결하여 번역 품질을 비약적으로 끌어올렸습니다.

## NER (Named Entity Recognition, 개체명 인식)와 BERT

### NER이란 무엇인가?

개체명 인식(NER)은 텍스트에서 사람 이름, 장소, 기관명, 날짜, 금액 등 특정 유형의 정보를 자동으로 찾아내고 분류하는 작업입니다.

**예시**:

- 문장: "김철수는 서울대학교에서 2024년 3월에 박사 학위를 받았다"
- NER 결과:
  - 김철수: 사람(PERSON)
  - 서울대학교: 기관(ORGANIZATION)
  - 2024년 3월: 날짜(DATE)

전통적인 NER 방법은 규칙 기반이나 간단한 머신러닝 방법을 사용했습니다. 하지만 이러한 방법들은 문맥을 재데로 고려하지 못하고 단어 의미를 잘못 파악합니다.

예를 들어, "서울"이라는 단어는 장소일 수도 있지만, "서울시장"이라는 단어에서는 기관명의 일부일 수도 있습니다. 문맥을 고려하지 않으면 이러한 구분이 어렵습니다.

BERT는 문맥을 고려하여 각 단어의 의미를 이해하므로, NER 작업에 매우 적합합니다. BERT는 각 단어를 처리할 때 문장의 전체 맥락을 고려하여, 그 단어가 어떤 유형의 개체명인지 정확하게 판단할 수 있습니다.

## Fuzzy Search (퍼지 검색)

Fuzzy Search는 오타나 철자 오류가 있어도 검색할 수 있는 검색 방법입니다. 사용자가 정확한 단어를 입력하지 않아도, 비슷한 단어를 찾아낼 수 있게 해줍니다.

**예시**:

- 사용자 입력: "서울대학교" (오타)
- Fuzzy Search 결과: "서울대학교"를 찾아냄

일상적인 비유로 설명하자면, Fuzzy Search는 "발음이 비슷한 이름을 찾아주는 친구"와 같습니다. 친구가 이름을 정확히 기억하지 못해도, 비슷하게 말하면 "아, 그 사람 말하는 거구나"라고 알아차리는 것과 같습니다.

### Fuzzy Search가 필요한 이유

사용자는 검색할 때 다음과 같은 오류를 범할 수 있습니다:

**오타**: "안녕하세요"를 "안녕하세여"로 입력

**철자 오류**: "컴퓨터"를 "컴퓨타"로 입력

**띄어쓰기 오류**: "서울대학교"를 "서울 대학교"로 입력

**다른 언어 입력**: "computer"를 "컴퓨터"로 입력하고 싶지만 영문으로 입력

전통적인 검색 방법은 정확히 일치하는 것만 찾기 때문에, 이러한 오류가 있으면 검색 결과를 찾을 수 없습니다. Fuzzy Search는 이러한 문제를 해결하여 사용자 경험을 크게 개선합니다.

### Fuzzy Search의 작동 원리

Fuzzy Search는 두 단어나 문자열이 얼마나 비슷한지를 측정하는 **유사도(Similarity)**를 계산합니다. 유사도가 일정 수준 이상이면 검색 결과로 반환합니다.

**주요 유사도 측정 방법**:

**1. 레벤슈타인 거리(Levenshtein Distance)**: 한 문자열을 다른 문자열로 변환하는 데 필요한 최소 편집 횟수(문자 추가, 삭제, 변경)를 계산합니다.

예를 들어:

- "고양이" → "고양이" (거리: 0, 완전히 같음)
- "고양이" → "고양일" (거리: 1, 한 글자 변경)
- "고양이" → "강아지" (거리: 3, 세 글자 변경)

거리가 작을수록 두 문자열이 더 비슷합니다.

**2. 음성 유사도(Phonetic Similarity)**: 발음이 비슷한 단어를 찾습니다. 철자가 달라도 발음이 비슷하면 검색 결과에 포함됩니다.

**3. 부분 일치(Partial Match)**: 문자열의 일부만 일치해도 검색 결과에 포함합니다.

### Fuzzy Search의 활용

Fuzzy Search는 다양한 분야에서 활용됩니다:

**검색 엔진**: 사용자의 오타에도 불구하고 원하는 결과를 찾아줌

**자동 완성(Autocomplete)**: 사용자가 입력하는 동안 비슷한 단어를 제안

**데이터베이스 검색**: 데이터베이스에서 오타가 있는 검색어도 찾아냄

**이름 검색**: 사람 이름을 검색할 때 오타나 다른 표기법도 인식

**제품 검색**: 온라인 쇼핑몰에서 제품명 검색 시 오타를 허용

**주소 검색**: 주소 검색 시 약어나 다른 표기를 인식

### Fuzzy Search의 한계와 주의사항

Fuzzy Search는 매우 유용하지만, 몇 가지 주의해야 할 점이 있습니다:

**성능**: 모든 문서를 검색하고 유사도를 계산해야 하므로, 데이터가 많을 때 느려질 수 있습니다. 이를 해결하기 위해 인덱싱이나 최적화 기법이 필요합니다.

**정확도**: 너무 관대하게 설정하면 관련 없는 결과도 많이 나올 수 있고, 너무 엄격하게 설정하면 오타가 있는 검색어를 찾지 못할 수 있습니다. 적절한 균형을 찾는 것이 중요합니다.

**언어 특성**: 한국어는 띄어쓰기가 중요하고, 영어와 다른 특성이 있어서 언어별로 다른 접근이 필요할 수 있습니다.

---

## 결론

자연어 처리(NLP)는 컴퓨터가 인간의 언어를 이해하고 처리할 수 있게 해주는 핵심 기술입니다. Word2Vec과 BERT 같은 현대적인 표현 방식은 단어와 문장의 의미를 더 정확하게 이해할 수 있게 해주었고, NER은 텍스트에서 중요한 정보를 자동으로 추출할 수 있게 해주었으며, Fuzzy Search는 사용자의 오류에도 불구하고 원하는 정보를 찾을 수 있게 해주었습니다.

### 핵심 요약:

1. **NLP의 목적**: 컴퓨터가 인간의 언어를 이해하고 처리
2. **Word2Vec**: 단어의 의미를 벡터로 표현, 유사한 의미의 단어가 가까운 벡터를 가짐
3. **BERT**: 문맥을 고려하여 같은 단어도 문맥에 따라 다른 벡터로 표현
4. **NER**: 텍스트에서 사람, 장소, 기관명 등 개체명을 자동으로 찾아내기
5. **Fuzzy Search**: 오타나 철자 오류가 있어도 비슷한 단어를 찾아내는 검색 방법

이러한 NLP 기술들은 현재 우리가 사용하는 번역 서비스, 검색 엔진, 챗봇, 음성 인식 등 다양한 AI 서비스의 기반이 되고 있습니다. NLP를 이해하면 이러한 서비스들이 어떻게 작동하는지 더 깊이 이해할 수 있습니다.
