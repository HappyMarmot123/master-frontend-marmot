## 1. 자연어 처리 개요

### (Natural Language Processing, NLP)의 정의와 역할

자연어 처리는 컴퓨터가 인간의 언어를 이해할 수 있도록 하는 인공지능 분야다. "자연어"란 한국어, 영어처럼 인간이 일상적으로 사용하는 언어를 말하며, 프로그래밍 언어 같은 인공 언어와 구분된다.

인간에게 "사과"라는 단어는 빨간 과일, 달콤한 맛, 건강에 좋다는 연상을 불러일으킨다. 하지만 컴퓨터에게 "사과"는 그저 문자의 나열일 뿐이다. 컴퓨터가 언어를 처리하려면 텍스트를 **숫자로 변환**해야 한다.

이 변환 과정의 핵심이 바로 **토큰화(Tokenization)**와 **임베딩(Embedding)**이다. 토큰화는 텍스트를 작은 단위로 쪼개고, 임베딩은 각 단위를 의미를 담은 숫자 벡터로 변환한다. 이렇게 변환된 숫자들을 신경망이 처리하여 언어를 "이해"하게 된다.

### 언어 모델과 다음 단어 예측

언어 모델(Language Model)은 텍스트의 확률 분포를 학습한 모델이다. 핵심 원리는 **다음에 올 단어를 예측**하는 것이다.

"오늘 날씨가"라는 문장이 주어지면, 언어 모델은 다음에 "좋다", "흐리다", "덥다" 등이 올 확률을 계산한다. 대규모 텍스트 데이터를 학습하면서 어떤 단어 뒤에 어떤 단어가 자주 오는지, 문맥에 따라 적절한 단어가 무엇인지를 파악한다.

## 2. 토큰화 (Tokenization)

### 토큰화의 정의와 필요성

토큰화(Tokenization)는 텍스트를 더 작은 단위인 **토큰(Token)**으로 분리하는 과정이다. 모델이 텍스트를 처리하기 위한 첫 단계로, 문장을 의미 있는 조각으로 나누어 각각에 숫자 ID를 부여한다.

"나는 학교에 간다"라는 문장을 처리한다고 가정하자. 이 문장을 그대로 모델에 넣을 수는 없다. 먼저 토큰으로 분리하고 ["나는", "학교에", "간다"] → [102, 5847, 3291] 처럼 숫자로 변환해야 한다.

토큰화가 중요한 이유는 **어휘 크기(Vocabulary Size)**와 **표현력** 사이의 균형 때문이다. 모든 단어를 개별 토큰으로 다루면 어휘가 너무 커지고, 글자 단위로 나누면 의미 파악이 어려워진다.

### 토큰화 방식

**단어 단위 토큰화**: 공백이나 구두점을 기준으로 단어를 분리한다. 직관적이지만, 신조어나 오타를 처리하지 못하는 OOV(Out-of-Vocabulary) 문제가 발생한다. "ChatGPT"라는 단어가 학습 데이터에 없으면 처리할 수 없다.

**문자 단위 토큰화**: 글자 하나하나를 토큰으로 취급한다. OOV 문제는 없지만, 시퀀스가 길어지고 의미 단위를 파악하기 어렵다. "안녕하세요"가 5개의 토큰이 되어 비효율적이다.

**서브워드 토큰화**: 단어와 문자의 중간 수준으로 분리한다. 자주 등장하는 문자 조합은 하나의 토큰으로, 드문 단어는 여러 조각으로 나눈다. "unhappiness"를 ["un", "happi", "ness"]로 분리하는 식이다. 현대 LLM의 표준 방식이다.

### BPE (Byte Pair Encoding)

BPE의 작동 원리는 다음과 같다. 처음에는 모든 문자를 개별 토큰으로 시작한다. 그다음 가장 자주 등장하는 토큰 쌍을 찾아 하나의 새로운 토큰으로 병합한다. 이 과정을 원하는 어휘 크기에 도달할 때까지 반복한다.

예를 들어 "low", "lower", "newest", "widest"라는 단어들이 있을 때, "e"와 "s"가 자주 함께 등장하면 "es"라는 토큰이 생성된다. 이렇게 자주 등장하는 패턴은 하나의 토큰이 되고, 드문 조합은 여러 토큰으로 분리된다.

## 3. 임베딩 (Embedding)

임베딩은 데이터의 의미와 특징을 숫자 벡터로 표현한 것이다. 단어, 문장, 문서, 심지어 이미지나 사용자 행동까지 벡터로 변환할 수 있다.

"왕"이라는 단어를 [0.2, -0.5, 0.8, ...] 같은 수백 차원의 벡터로 표현한다고 생각하면 된다. 이 벡터의 각 차원은 특정 의미적 특성을 나타낸다. 중요한 점은 **의미가 비슷한 단어는 비슷한 벡터**를 갖는다는 것이다.

임베딩 덕분에 컴퓨터는 "왕"과 "여왕"이 비슷하고, "왕"과 "사과"는 다르다는 것을 수치적으로 판단할 수 있다. 벡터 사이의 거리를 계산하면 의미적 유사도를 측정할 수 있기 때문이다.

임베딩 벡터는 **고차원 공간**에 위치한다. 일반적으로 수백에서 수천 차원을 사용한다. 고차원 공간에서 의미가 비슷한 단어들은 가까이 모여 있다. 

"강아지", "고양이", "햄스터"는 서로 가깝고, "자동차", "비행기", "기차"도 서로 가깝다. 
하지만 "강아지"와 "자동차"는 멀리 떨어져 있다.

흥미로운 점은 임베딩 공간에서 **의미적 연산**이 가능하다는 것이다. "왕 - 남자 + 여자 ≈ 여왕"처럼 벡터 연산으로 의미 관계를 표현할 수 있다. 이는 임베딩이 단순한 숫자 변환이 아니라 의미 구조를 포착하고 있음을 보여준다.

### 단어 임베딩에서 문장 임베딩으로

초기 임베딩 기법인 Word2Vec이나 FastText는 **단어 단위**로 벡터를 생성했다. 하지만 같은 단어도 문맥에 따라 의미가 다르다. "배가 고프다"와 "배가 항구에 있다"에서 "배"는 전혀 다른 의미다.

현대 임베딩 모델은 **문맥을 고려한 임베딩**을 생성한다. BERT, GPT 기반 모델들은 전체 문장을 보고 각 단어의 임베딩을 결정한다. 같은 "배"라도 주변 단어에 따라 다른 벡터가 생성된다.

임베딩의 핵심 가치는 **벡터 간 거리를 계산**할 수 있다는 것이다. 이를 통해 다양한 실용적 작업이 가능해진다.

### 검색 및 추천

시맨틱 검색(Semantic Search)은 임베딩의 대표적인 활용 사례다. 전통적인 키워드 검색은 "맛있는 파스타 레시피"를 검색하면 정확히 그 단어가 포함된 문서만 찾는다. 하지만 시맨틱 검색은 "이탈리안 면요리 만들기"처럼 의미가 비슷한 문서도 찾아낸다.

검색 쿼리와 문서를 모두 임베딩으로 변환한 뒤, 가장 가까운 문서를 반환한다. 추천 시스템도 같은 원리다. 사용자가 본 상품과 임베딩이 가까운 상품을 추천한다.

### 클러스터링 및 분류

임베딩 공간에서 가까운 데이터는 비슷한 의미를 가진다. 이를 이용해 비슷한 문서를 자동으로 그룹화(클러스터링)할 수 있다. 수천 개의 고객 리뷰를 임베딩한 뒤 클러스터링하면, "배송 관련", "품질 관련", "가격 관련" 등으로 자동 분류된다.

분류 작업도 마찬가지다. 학습 데이터의 임베딩 패턴을 파악하면, 새로운 텍스트가 어떤 카테고리에 속하는지 예측할 수 있다.

### 이상치 감지

임베딩 공간에서 다른 데이터와 거리가 먼 데이터는 **이상치**일 가능성이 높다. 대부분의 이메일은 비슷한 영역에 모여 있는데, 유독 멀리 떨어진 이메일이 있다면 스팸이나 피싱일 수 있다.

## 5. 유사도 계산

### 유클리드 거리 (Euclidean Distance)

벡터의 크기가 중요할 때 사용한다. 예를 들어 사용자의 구매 금액을 임베딩에 포함했다면, 비슷한 소비 패턴과 비슷한 금액대의 사용자를 찾고 싶을 수 있다.

유클리드 거리는 두 점 사이의 **직선 거리**다. 2차원 평면에서 두 점 (x1, y1)과 (x2, y2) 사이의 거리를 계산하는 피타고라스 정리의 확장이다. 고차원에서도 같은 원리로 계산한다. 각 차원의 차이를 제곱하여 더한 뒤 제곱근을 취한다. 거리가 작을수록 두 벡터가 비슷하다는 의미다. 유클리드 거리는 **벡터의 크기에 영향**을 받는다. 같은 방향을 가리키더라도 길이가 다르면 거리가 멀어진다. 이 특성이 장점이 될 수도, 단점이 될 수도 있다.

### 코사인 유사도 (Cosine Similarity)

벡터의 방향(의미)만 중요할 때 사용한다. 텍스트 유사도, 이미지 유사도 등 대부분의 시맨틱 검색에서 선호된다.

코사인 유사도는 두 벡터 사이의 **각도**를 측정한다. 벡터의 크기는 무시하고 방향만 비교한다. 코사인 유사도는 -1에서 1 사이의 값을 가진다. 1은 완전히 같은 방향(동일한 의미), 0은 직교(관련 없음), -1은 반대 방향(반대 의미)을 나타낸다.

### 좋은 임베딩 모델의 조건

좋은 임베딩 모델은 **의미가 비슷한 것은 가깝게, 다른 것은 멀리** 배치한다. 검색 쿼리 "파이썬 웹 개발"에 대해 "Django 튜토리얼", "Flask 시작하기" 같은 문서가 높은 유사도를 보여야 한다.

임베딩 모델의 품질은 도메인에 따라 다르다. 일반 텍스트로 학습된 모델이 법률 문서나 의학 논문에서는 성능이 떨어질 수 있다. 이런 경우 해당 도메인 데이터로 **추가 학습(Fine-tuning)**하면 성능이 향상된다. 한국어의 경우 KLUE의 MRC 데이터셋 등으로 미세조정하여 한국어 검색 성능을 높일 수 있다.

## 6. 벡터 데이터베이스

벡터를 효율적으로 저장하고, 빠르게 유사한 벡터를 검색할 수 있도록 최적화된 데이터베이스다.

벡터 DB는 정확한 최근접 이웃 대신 "거의 가까운" 이웃을 빠르게 찾는다. 약간의 정확도를 희생하고 속도를 크게 향상시킨다.

벡터 DB의 일반적인 워크플로우는 다음과 같다. 먼저 문서들을 임베딩하여 벡터 DB에 저장한다. 검색 시 쿼리를 임베딩하고, 벡터 DB에서 가장 가까운 K개의 벡터를 조회한다. 조회된 벡터에 연결된 원본 문서를 반환한다.