# 확률과 통계 (Probability and Statistics)

확률과 통계는 인공지능이 데이터를 분석적으로 이해하고, 불확실성을 수치화하며, 모델의 성능과 예측의 신뢰도를 평가하는 데 필수적인 학문입니다. AI는 본질적으로 확률적 모델이며, 데이터를 기반으로 최적의 예측을 수행하기 위해 통계적 원리를 활용합니다.

일상생활에서도 확률과 통계의 개념을 쉽게 찾아볼 수 있습니다. 예를 들어, 날씨 앱이 "내일 비올 확률 70%"라고 예보한다면, 이것은 통계적 분석을 바탕으로 한 예측입니다. AI 모델도 마찬가지입니다. 이미지 분류 모델이 "이 사진이 고양이일 확률 95%, 강아지일 확률 3%, 기타 2%"라고 출력한다면, 이것은 확률 분포의 형태로 예측을 표현한 것입니다.

---

## 1. 데이터 분포 (Data Distribution): 데이터의 경향성 파악

### 분포의 개념

데이터 분포는 특정 데이터 값이 나타날 가능성이나 빈도를 나타내는 패턴입니다. 쉽게 말해, "어떤 값이 얼마나 자주 나타나는가?"를 그래프로 표현한 것이 분포입니다.

예를 들어, 한 반의 학생 30명의 시험 점수를 모았다고 가정해봅시다. 점수들을 그래프로 그려보면, 대부분의 학생이 70-80점 사이에 몰려 있고, 50점 이하나 90점 이상을 받은 학생은 적을 것입니다. 이렇게 점수들이 분포하는 패턴이 바로 "데이터 분포"입니다.

AI 모델을 구축하기 위해서는 학습 데이터가 어떤 분포를 따르는지 이해하는 것이 매우 중요합니다. 데이터의 분포를 이해하지 못하면, 모델이 잘못된 패턴을 학습하거나 특정 값에만 치우친 예측을 할 수 있기 때문입니다.

### 정규분포 (Normal Distribution)

정규분포는 종 모양의 대칭적인 분포로, 자연 현상과 데이터 과학 분야에서 가장 흔하게 가정되는 분포입니다. 우리 주변의 많은 현상들이 정규분포를 따릅니다: 사람의 키, 시험 점수, 측정 오차 등.

정규분포의 특징:

- **대칭성**: 중심값(평균)을 기준으로 좌우 대칭입니다. 평균값 주변의 데이터가 가장 많고, 평균에서 멀어질수록 데이터가 적어집니다.
- **종 모양**: 그래프가 종 모양의 곡선을 그립니다.
- **평균과 표준편차**: 평균은 분포의 중심 위치를, 표준편차는 데이터가 얼마나 퍼져 있는지를 나타냅니다. 표준편차가 클수록 데이터가 더 넓게 분산되어 있습니다.

### 분포의 실제 활용

데이터 분포를 이해하면 다음과 같은 문제를 해결할 수 있습니다:

**이상치 감지**: 분포에서 비정상적으로 멀리 떨어진 값을 이상치로 판단할 수 있습니다. 예를 들어, 대부분의 고객이 월 100-500만원을 소비하는데, 한 고객만 1억원을 소비했다면 이것은 이상치일 가능성이 높습니다.

**데이터 편향 식별**: 데이터가 한쪽으로 치우쳐 있다면(왜도), 모델이 편향된 학습을 할 수 있습니다. 이 경우 데이터를 균형 있게 조정하거나, 모델 학습 방법을 변경해야 합니다.

**확률적 예측**: 분류 문제에서 모델의 최종 출력은 각 클래스에 속할 확률의 형태로 나타납니다. 예를 들어, 이메일 스팸 필터는 "이 이메일이 스팸일 확률 85%, 정상 메일일 확률 15%"와 같이 확률 분포로 표현합니다.

```python
import numpy as np
import matplotlib.pyplot as plt

# 정규분포 데이터 생성 (평균 75, 표준편차 10)
# 시험 점수 시뮬레이션
scores = np.random.normal(loc=75, scale=10, size=1000)

# 통계 요약
mean_score = np.mean(scores)
std_score = np.std(scores)
print(f"평균 점수: {mean_score:.2f}")
print(f"표준편차: {std_score:.2f}")

# 이상치 감지: 평균에서 3표준편차 이상 벗어난 값
outliers = scores[(scores < mean_score - 3*std_score) |
                  (scores > mean_score + 3*std_score)]
print(f"이상치 개수: {len(outliers)}")
```

---

## 2. 통계적 추론 (Statistical Inference): 예측의 신뢰도 확보

### 통계적 추론의 개념

통계적 추론은 관찰 가능한 표본 데이터의 정보를 바탕으로, 미지의 전체 집단(모집단)에 대한 특성을 추정하고 가설을 검증하는 과정입니다.

실생활 예시로 설명하면, 전국 고등학생의 평균 키를 알고 싶다고 가정해봅시다. 전국의 모든 고등학생을 측정하는 것은 불가능하므로, 무작위로 선택한 1000명의 키를 측정합니다. 이 1000명의 데이터는 "표본"이고, 전국의 모든 고등학생은 "모집단"입니다. 표본의 평균 키를 계산하여, 이를 바탕으로 모집단의 평균 키를 추정하는 것이 통계적 추론입니다.

AI에서 통계적 추론이 중요한 이유는, 모델의 예측이 단순한 우연이 아닌 통계적으로 유의미한 결과임을 입증해야 하기 때문입니다. 모델의 정확도가 95%라고 할 때, 이것이 정말로 좋은 성능인지, 아니면 우연히 좋은 결과가 나온 것인지를 통계적으로 판단해야 합니다.

### 가설 검정 (Hypothesis Testing)

가설 검정은 데이터를 바탕으로 특정 주장이 맞는지 틀린지를 통계적으로 판단하는 방법입니다. 모델의 성능 개선 여부나 특정 변수의 효과를 검증할 때 가설 검정 방법을 사용합니다.

예를 들어, 새로운 학습 알고리즘을 개발했다고 가정해봅시다. 기존 알고리즘의 정확도가 90%이고, 새 알고리즘의 정확도가 92%입니다. 이 2%의 차이가 실제로 알고리즘 개선으로 인한 것인지, 아니면 단순히 우연히 발생한 것인지를 가설 검정을 통해 판단할 수 있습니다.

### 표본과 모집단

통계적 추론의 핵심은 "작은 표본으로부터 큰 모집단에 대한 결론을 도출"하는 것입니다. 하지만 여기에는 한계가 있습니다. 표본이 너무 작으면 모집단을 대표하지 못할 수 있고, 표본이 편향되어 있으면 잘못된 결론을 내릴 수 있습니다.

AI 모델 학습에서도 같은 원리가 적용됩니다. 학습 데이터는 "표본"이고, 모델이 예측해야 할 실제 데이터는 "모집단"입니다. 학습 데이터가 실제 데이터를 잘 대표하지 못하면, 모델은 학습 데이터에는 잘 맞지만 실제 데이터에서는 성능이 떨어지는 문제(과적합)가 발생합니다.

```python
import numpy as np
from scipy import stats

# 두 모델의 성능 비교 예제
# 모델 A와 모델 B의 정확도를 여러 번 측정
model_a_scores = np.array([0.90, 0.91, 0.89, 0.92, 0.90, 0.91])
model_b_scores = np.array([0.92, 0.93, 0.91, 0.94, 0.92, 0.93])

# 평균 비교
mean_a = np.mean(model_a_scores)
mean_b = np.mean(model_b_scores)
print(f"모델 A 평균: {mean_a:.3f}")
print(f"모델 B 평균: {mean_b:.3f}")

# t-검정: 두 모델의 성능에 유의미한 차이가 있는지 검정
t_statistic, p_value = stats.ttest_rel(model_a_scores, model_b_scores)
print(f"\n검정 결과:")
print(f"p-value: {p_value:.4f}")
if p_value < 0.05:
    print("두 모델의 성능 차이는 통계적으로 유의미합니다.")
else:
    print("두 모델의 성능 차이는 통계적으로 유의미하지 않습니다.")
```

---

## 3. 베이즈 정리 (Bayes' Theorem): 새로운 증거로 믿음 업데이트

### 베이즈 정리의 개념

베이즈 정리는 AI 분야에서 특히 중요한 개념으로, 새로운 증거(데이터)를 관찰했을 때 기존의 믿음(사전 확률)을 업데이트하여 새로운 결론(사후 확률)을 계산하는 방법입니다.

일상생활 예시로 설명하면, 의사가 환자를 진단하는 과정을 생각해볼 수 있습니다:

- **사전 확률**: 환자가 특정 질병을 가지고 있을 가능성 (질병의 일반적인 유병률)
- **증거**: 검사 결과 (예: 양성 반응)
- **사후 확률**: 검사 결과를 확인한 후, 환자가 실제로 질병을 가지고 있을 확률

베이즈 정리는 "증거를 본 후에 믿음을 업데이트"하는 공식이라고 이해할 수 있습니다.

### 베이즈 정리의 수식과 의미

베이즈 정리의 기본 공식은 다음과 같습니다:

**P(질병|검사 양성) = P(검사 양성|질병) × P(질병) / P(검사 양성)**

이 공식의 의미를 풀어보면:

- **P(질병)**: 사전 확률 - 검사 전에 질병을 가지고 있을 확률
- **P(검사 양성|질병)**: 질병이 있을 때 검사가 양성일 확률 (민감도)
- **P(질병|검사 양성)**: 사후 확률 - 검사 결과를 본 후 질병을 가지고 있을 확률

### AI에서의 베이즈 정리 활용

베이즈 정리는 불확실성이 존재하는 상황에서 확률적으로 가장 합리적인 결정을 내리는 기초가 되며, 베이지안 머신러닝 모델의 근간을 이룹니다.

**스팸 필터 예시**: 이메일이 스팸일 확률을 계산할 때, 베이즈 정리를 사용합니다. 이메일에 특정 단어들(예: "무료", "당첨")이 포함되어 있다는 증거를 바탕으로, 이메일이 스팸일 사전 확률을 업데이트하여 최종적인 스팸 확률을 계산합니다.

**추천 시스템**: 사용자가 특정 영화를 좋아할 확률을 계산할 때, 사용자의 과거 관람 기록(증거)을 바탕으로 사전 확률을 업데이트합니다.

베이즈 정리의 핵심 철학은 "확률은 주관적 믿음의 정도"라는 것입니다. 데이터를 더 많이 관찰할수록, 우리의 믿음은 더 정확해지고 불확실성은 줄어듭니다. 이것이 바로 머신러닝 모델이 학습하는 과정과도 일치합니다.

```python
# 베이즈 정리 예제: 스팸 이메일 필터
# 사전 확률: 전체 이메일 중 스팸 비율
p_spam = 0.3  # 30%의 이메일이 스팸
p_not_spam = 0.7  # 70%의 이메일이 정상

# 조건부 확률: 특정 단어가 나타날 확률
# P("당첨"|스팸): 스팸일 때 "당첨" 단어가 나타날 확률
p_word_given_spam = 0.8  # 스팸의 80%에 "당첨" 포함
p_word_given_not_spam = 0.1  # 정상 메일의 10%에 "당첨" 포함

# 증거: "당첨" 단어가 포함된 이메일
# 베이즈 정리를 사용하여 사후 확률 계산
# P(스팸|"당첨") = P("당첨"|스팸) × P(스팸) / P("당첨")
p_word = p_word_given_spam * p_spam + p_word_given_not_spam * p_not_spam
p_spam_given_word = (p_word_given_spam * p_spam) / p_word

print(f"사전 확률 (스팸일 확률): {p_spam:.1%}")
print(f"증거 발견: '당첨' 단어 포함")
print(f"사후 확률 (스팸일 확률): {p_spam_given_word:.1%}")
```
