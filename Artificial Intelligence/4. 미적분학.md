# 미적분학 (Calculus)

미적분학은 인공지능, 특히 딥러닝 모델이 가장 효율적인 방식으로 학습하고 성능을 개선하는 과정을 수학적으로 정의합니다. 모델이 학습하는 행위는 본질적으로 최적의 가중치(매개변수)를 찾는 최적화(Optimization) 문제이며, 이는 미분을 통해 해결됩니다.

일상적인 비유로 설명하자면, 미적분학은 AI 모델이 "어떤 방향으로, 얼마나 빠르게 개선해야 할지"를 알려주는 나침반과 속도계와 같은 역할을 합니다. 눈을 가리고 산 정상에서 내려와야 하는 상황을 생각해봅시다. 다행히 바닥의 기울기를 느낄 수 있다면, 기울기가 가장 가파른 방향으로 조금씩 내려가면 결국 산 아래 도착할 수 있습니다. 경사하강법(Gradient Descent)도 같은 원리입니다. 모델은 현재 위치에서 "오차가 가장 빠르게 줄어드는 방향"을 계산하고, 그 방향으로 조금씩 이동하여 최적의 지점을 찾아갑니다.

---

## 1. 손실 함수와 최적화 문제

### 손실 함수란 무엇인가?

AI 모델은 예측 결과와 실제 정답 사이의 차이를 계산하는 **손실 함수(Loss Function)**를 정의합니다. 손실 함수는 모델이 얼마나 잘못 예측했는지를 수치로 나타냅니다. 손실 값이 크면 모델의 예측이 실제와 많이 다르다는 의미이고, 손실 값이 작으면 예측이 정확하다는 의미입니다.

예를 들어, 집 가격 예측 모델이 있다고 가정해봅시다. 실제 집 가격이 5억원인데 모델이 3억원으로 예측했다면, 오차는 2억원입니다. 만약 모델이 4.9억원으로 예측했다면, 오차는 0.1억원입니다. 두 번째 예측이 훨씬 정확하므로 손실 값도 더 작습니다.

### 최적화의 어려움

모델의 목표는 이 손실 함수의 값을 최소화하는 가중치 조합을 찾는 것입니다. 문제는 가중치의 조합이 무한히 많다는 것입니다. 각 가중치가 가질 수 있는 값이 무수히 많기 때문에, 모든 조합을 일일이 시도해보는 것은 불가능합니다. 예를 들어, 가중치가 10개만 있어도 각 가중치가 100가지 값을 가질 수 있다면, 총 100의 10제곱 = 100,000,000,000,000,000,000,000,000,000,000 가지의 조합이 생깁니다. 이것을 모두 시도하는 것은 현실적으로 불가능합니다.

따라서 우리는 더 스마트한 방법이 필요합니다. 여기서 미적분학이 등장합니다. 미분을 사용하면, 각 가중치를 어떤 방향으로 조정해야 손실이 줄어드는지 계산할 수 있습니다. 이것이 바로 경사하강법의 핵심 아이디어입니다.

```python
import numpy as np

# 간단한 손실 함수 예제: 예측값과 실제값의 차이 제곱
# 실제 집 가격
actual_prices = np.array([5.0, 3.0, 4.5])  # 억원 단위

# 모델의 예측값 (초기에는 부정확함)
predicted_prices = np.array([3.0, 2.0, 3.0])

# 평균 제곱 오차 (Mean Squared Error) 계산
def mean_squared_error(actual, predicted):
    """손실 함수: 예측 오차의 제곱 평균"""
    return np.mean((actual - predicted) ** 2)

loss = mean_squared_error(actual_prices, predicted_prices)
print(f"현재 손실 값: {loss:.2f}")

# 예측이 개선된 경우
improved_predictions = np.array([4.9, 2.9, 4.4])
improved_loss = mean_squared_error(actual_prices, improved_predictions)
print(f"개선된 손실 값: {improved_loss:.2f}")
print(f"손실 감소: {loss - improved_loss:.2f}")
```

---

## 2. 미분과 기울기의 개념

### 미분이란 무엇인가?

미분은 함수의 기울기(경사)를 계산하는 수학적 도구입니다. 어떤 함수가 주어졌을 때, 특정 지점에서 그 함수가 얼마나 빠르게 변하는지를 알려줍니다.

일상생활에서 기울기의 개념을 생각해봅시다. 언덕을 오르는 길에서, 경사가 가파를수록 올라가기 어렵고 힘이 많이 듭니다. 수학적으로도 마찬가지입니다. 함수 그래프에서 기울기가 크다는 것은 값이 빠르게 변한다는 의미입니다.

### 편미분의 개념

AI에서 중요한 것은 편미분(Partial Differentiation)입니다. 손실 함수는 수천 개의 가중치에 의존하는 매우 복잡한 함수입니다. 편미분은 여러 변수 중 하나만 변화시키고 나머지는 고정시킨 상태에서, 그 변수가 손실 값에 얼마나 영향을 미치는지를 계산합니다.

예를 들어, 모델에 가중치가 3개(w1, w2, w3)만 있다고 가정해봅시다.w2와 w3는 그대로 두고 w1만 약간 변화시켰을 때 손실 값이 얼마나 변하는지를 알 수 있습니다.

만약 w1에 대한 편미분이 큰 양수라면, w1을 조금 증가시키면 손실이 크게 증가한다는 의미입니다. 따라서 손실을 줄이려면 w1을 감소시켜야 합니다. 반대로 편미분이 큰 음수라면, w1을 증가시키면 손실이 줄어듭니다.

### 경사(Gradient)의 의미

**경사(Gradient)**는 모든 가중치에 대한 편미분을 모아놓은 벡터입니다. 경사는 현재 위치에서 손실 값이 가장 빠르게 증가하는 방향을 가리킵니다. 따라서 손실을 줄이려면 경사의 반대 방향으로 이동해야 합니다.

산 정상에서 내려오는 비유를 다시 생각해봅시다. 현재 위치에서 바닥의 기울기를 느껴보면, 가장 가파른 오르막 방향을 알 수 있습니다. 내려오려면 그 반대 방향(가장 가파른 내리막 방향)으로 가야 합니다. 경사하강법도 똑같습니다. 경사가 가리키는 반대 방향으로 가중치를 조정하면 손실이 줄어듭니다.

```python
import numpy as np

# 간단한 함수: f(x) = x^2
# 최솟값은 x=0에서 발생
def simple_function(x):
    """간단한 2차 함수"""
    return x ** 2

def derivative(x):
    """미분: f'(x) = 2x"""
    return 2 * x

# 현재 위치에서 경사(기울기) 계산
x_current = 3.0  # 현재 위치
gradient = derivative(x_current)
print(f"현재 위치: x = {x_current}")
print(f"현재 함수 값: f({x_current}) = {simple_function(x_current):.2f}")
print(f"경사(기울기): {gradient:.2f}")

# 경사하강: 경사의 반대 방향으로 이동
learning_rate = 0.1  # 이동하는 크기
x_new = x_current - learning_rate * gradient
print(f"\n경사하강 후 위치: x = {x_new:.2f}")
print(f"새 함수 값: f({x_new:.2f}) = {simple_function(x_new):.2f}")
print(f"개선됨: {simple_function(x_current) - simple_function(x_new):.2f}만큼 감소")
```

---

## 3. 경사하강법 (Gradient Descent): 학습의 엔진

### 경사하강법의 원리

경사하강법은 손실 함수의 최솟값을 찾기 위해 사용되는 핵심 최적화 알고리즘입니다. 그 원리는 간단합니다:

1. **경사 계산**: 현재 가중치에서 손실 함수의 경사를 계산합니다. 이 경사는 각 가중치를 어느 방향으로 조정해야 손실이 줄어드는지를 알려줍니다.

2. **가중치 업데이트**: 계산된 경사의 반대 방향으로 모델의 가중치를 일정한 크기(학습률, Learning Rate)만큼 이동시킵니다. 경사가 양수이면 가중치를 줄이고, 경사가 음수이면 가중치를 늘립니다.

3. **반복**: 이 과정을 수천, 수만 번 반복하면서 가중치를 점진적으로 조정하여 손실 함수의 최솟값(가장 좋은 모델 성능)에 도달하게 합니다.

### 학습률의 중요성

**학습률(Learning Rate)**은 각 단계에서 가중치를 얼마나 크게 변경할지를 결정하는 매우 중요한 하이퍼파라미터입니다.

학습률이 너무 크면: 가중치가 너무 크게 변경되어 최적점을 지나쳐버릴 수 있습니다. 마치 계단을 내려올 때 한 번에 너무 많은 계단을 내려와서 목표 지점을 놓치는 것과 같습니다. 더 나쁜 경우, 손실 값이 발산하여 점점 커질 수도 있습니다.

학습률이 너무 작으면: 가중치가 거의 변하지 않아서 학습이 매우 느리게 진행됩니다. 최적점에 도달하려면 수백만 번의 반복이 필요할 수도 있습니다. 이는 매우 작은 보폭으로 산을 내려오는 것과 같아서, 시간이 너무 오래 걸립니다.

적절한 학습률은 보통 실험을 통해 찾아야 하지만, 일반적으로 0.001부터 0.1 사이의 값을 사용합니다.

### 경사하강법의 한계와 개선

기본적인 경사하강법은 모든 데이터를 사용하여 경사를 계산합니다(배치 경사하강법). 하지만 데이터가 수백만 개일 경우, 한 번의 업데이트를 위해 모든 데이터를 처리해야 하므로 시간이 너무 오래 걸립니다.

따라서 실제로는 **확률적 경사하강법(Stochastic Gradient Descent)** 또는 **미니배치 경사하강법(Mini-batch Gradient Descent)**을 사용합니다. 이 방법들은 데이터의 일부만 사용하여 경사를 근사적으로 계산하지만, 계산 속도가 훨씬 빠르고 대부분의 경우 비슷하게 좋은 결과를 얻을 수 있습니다.

### 경사하강법의 비유

경사하강법을 이해하는 가장 좋은 비유는 눈을 가리고 산에서 내려오는 것입니다:

- 손실 함수는 산의 지형입니다
- 현재 가중치는 현재 위치입니다
- 경사는 바닥의 기울기입니다
- 학습률은 한 번에 내려오는 보폭입니다
- 최적점은 산 아래의 마을(손실이 최소인 지점)입니다

눈을 가린 채로 기울기만 느껴가며 내려오면, 결국 산 아래 도착할 수 있습니다. 마찬가지로 경사하강법도 경사만 계산하며 가중치를 조정하면, 결국 손실 함수의 최솟값에 도달합니다.

```python
import numpy as np
import matplotlib.pyplot as plt

# 간단한 1차원 최적화 문제: f(x) = (x - 3)^2 + 2
# 최솟값은 x=3에서 발생하며, 값은 2입니다.
def loss_function(x):
    """손실 함수"""
    return (x - 3) ** 2 + 2

def gradient(x):
    """경사 계산: f'(x) = 2(x - 3)"""
    return 2 * (x - 3)

# 경사하강법 구현
x = 8.0  # 초기 위치
learning_rate = 0.2
iterations = 20

history = []  # 위치와 손실 값을 기록

for i in range(iterations):
    loss = loss_function(x)
    grad = gradient(x)
    history.append((x, loss))

    # 경사의 반대 방향으로 이동
    x = x - learning_rate * grad

    if i % 5 == 0:
        print(f"반복 {i}: x={x:.3f}, 손실={loss:.3f}, 경사={grad:.3f}")

print(f"\n최종 위치: x={x:.3f}")
print(f"최종 손실: {loss_function(x):.3f}")
print(f"실제 최소값 위치: x=3.0, 손실=2.0")
```

---

## 4. 역전파 알고리즘 (Backpropagation): 미분 계산의 혁신

### 역전파가 필요한 이유

신경망은 수많은 층(Layer)으로 이루어져 있습니다. 입력층에서 데이터가 들어와서 여러 은닉층을 거쳐 출력층까지 전달되고, 그 결과 손실이 계산됩니다. 문제는 손실 함수가 모든 층의 모든 가중치에 의존한다는 것입니다.

예를 들어, 3층 신경망이 있고 각 층에 가중치가 100개씩 있다고 가정해봅시다. 총 300개의 가중치가 있고, 손실 함수는 이 300개 가중치 모두의 함수입니다. 각 가중치에 대한 편미분을 모두 계산하려면, 엄청난 계산량이 필요합니다.

### 연쇄 법칙 (Chain Rule)

역전파 알고리즘은 미적분학의 **연쇄 법칙(Chain Rule)**을 응용합니다. 연쇄 법칙은 복잡한 함수의 미분을 단순한 함수들의 미분의 곱으로 계산할 수 있게 해줍니다.

예를 들어, 함수 f(g(x))가 있다고 하면, 이 함수를 x에 대해 미분하면 f'(g(x)) × g'(x)가 됩니다. 즉, 외부 함수의 미분과 내부 함수의 미분을 곱하면 됩니다.

신경망에서도 마찬가지입니다. 출력층의 손실은 마지막 층의 출력에 의존하고, 마지막 층의 출력은 그 전 층의 출력에 의존하며, 이런 식으로 계속 거슬러 올라갑니다. 따라서 각 층의 미분을 역방향으로 곱해가면, 모든 가중치에 대한 편미분을 효율적으로 계산할 수 있습니다.

### 역전파의 작동 방식

역전파 알고리즘은 다음과 같이 작동합니다:

1. **순전파(Forward Pass)**: 입력 데이터가 신경망을 통과하여 출력이 나오고, 손실이 계산됩니다.

2. **역전파(Backward Pass)**: 출력층에서 계산된 오차(손실의 미분)를 시작으로, 각 층을 거꾸로 돌아가면서 다음을 수행합니다:

   - 현재 층의 출력에 대한 손실의 미분을 계산
   - 연쇄 법칙을 사용하여 이전 층으로 오차를 전파
   - 각 가중치에 대한 편미분을 계산

3. **가중치 업데이트**: 계산된 모든 가중치의 편미분(경사)을 사용하여 경사하강법으로 가중치를 업데이트합니다.

### 역전파의 효율성

역전파 알고리즘의 혁신적인 점은, 한 번의 역방향 통과만으로 모든 가중치에 대한 경사를 동시에 계산할 수 있다는 것입니다. 만약 각 가중치를 개별적으로 미분하려면, 신경망을 수만 번, 수십만 번 순전파해야 합니다. 하지만 역전파는 한 번의 역방향 계산으로 모든 경사를 얻을 수 있습니다.

### 역전파의 비유

역전파를 이해하는 좋은 비유는 전화 통화입니다:

- 순전파: 발신자가 메시지를 여러 중계소를 거쳐 수신자에게 전달
- 손실 계산: 수신자가 메시지를 이해했는지 확인
- 역전파: 오류가 발생했다면, 어느 중계소에서 문제가 생겼는지 역방향으로 추적하여 각 중계소에 피드백 전달

이렇게 각 중계소(층)가 자신의 역할을 개선할 수 있게 됩니다.

```python
import numpy as np

# 간단한 2층 신경망 역전파 시뮬레이션
# 입력 → 은닉층 → 출력
# 연쇄 법칙을 사용한 미분 계산

def simple_forward(input_val, w1, w2):
    """순전파: 입력이 두 층을 통과"""
    hidden = input_val * w1  # 첫 번째 층
    output = hidden * w2     # 두 번째 층
    return hidden, output

def simple_backward(input_val, w1, w2, target):
    """역전파: 연쇄 법칙을 사용하여 경사 계산"""
    hidden, output = simple_forward(input_val, w1, w2)
    loss = (output - target) ** 2  # 손실 함수

    # 출력층의 오차
    dloss_doutput = 2 * (output - target)

    # 연쇄 법칙: 출력 → 은닉층
    dloss_dhidden = dloss_doutput * w2

    # 연쇄 법칙: 은닉층 → 가중치 w2
    dloss_dw2 = dloss_doutput * hidden

    # 연쇄 법칙: 은닉층 → 가중치 w1
    dloss_dw1 = dloss_dhidden * input_val

    return dloss_dw1, dloss_dw2, loss

# 예제 실행
input_val = 2.0
w1 = 0.5
w2 = 1.5
target = 3.0

grad_w1, grad_w2, loss = simple_backward(input_val, w1, w2, target)
print(f"입력: {input_val}")
print(f"가중치 w1: {w1}, w2: {w2}")
print(f"목표값: {target}")
print(f"\n경사 계산 (역전파):")
print(f"w1에 대한 경사: {grad_w1:.3f}")
print(f"w2에 대한 경사: {grad_w2:.3f}")
print(f"현재 손실: {loss:.3f}")

# 경사하강법으로 가중치 업데이트
learning_rate = 0.1
w1_new = w1 - learning_rate * grad_w1
w2_new = w2 - learning_rate * grad_w2
print(f"\n업데이트 후:")
print(f"w1: {w1} → {w1_new:.3f}")
print(f"w2: {w2} → {w2_new:.3f}")
```
