## 1. 환각 현상 (Hallucination)

환각은 LLM이 사실이 아닌 정보를 마치 사실인 것처럼 생성하는 현상이다. 모델이 자신 있게 답변하지만, 그 내용이 틀렸거나 존재하지 않는 정보인 경우를 말한다. LLM은 학습 데이터에 포함된 패턴을 학습한다. 학습 데이터에 오류가 있거나, 특정 주제에 대한 정보가 부족하면 잘못된 답변을 생성할 수 있다. 모델은 학습 시점까지의 정보만 알고 있다. 최신 정보나 학습 이후 발생한 사건에 대해서는 정확한 답변을 할 수 없다. 환각은 LLM의 신뢰성을 크게 훼손한다. 특히 의료/법률과 같은 전문 분야에서 심각한 문제가 된다.

## 2. RAG (Retrieval-Augmented Generation, 검색 증강 생성)

RAG는 외부 지식을 검색하여 LLM의 응답에 활용하는 기법이다. 모델이 자체 파라미터에 저장된 지식에만 의존하지 않고, 실시간으로 관련 문서를 찾아 참조한다. RAG의 핵심 아이디어는 간단하다. 사용자 질문에 답하기 전에, 먼저 관련된 정보를 데이터베이스에서 검색한다. 검색된 정보를 질문과 함께 모델에 제공하면, 모델은 이 정보를 바탕으로 더 정확한 답변을 생성한다.

RAG는 세 단계로 작동한다:

- **검색(Retrieval)**: 사용자 질문과 관련된 문서를 찾는다. 질문을 임베딩으로 변환하고, 벡터 데이터베이스에서 유사한 임베딩을 가진 문서를 검색한다. 상위 K개의 관련 문서를 선택한다.
- **증강(Augmentation)**: 검색된 문서를 사용자 질문과 결합하여 프롬프트를 구성한다. "다음 문서를 참고하여 질문에 답하세요: [검색된 문서들] 질문: [사용자 질문]" 형태로 만든다.
- **생성(Generation)**: 증강된 프롬프트를 LLM에 입력하여 응답을 생성한다. 모델은 제공된 문서의 정보를 활용하여 답변한다.

### RAG 파이프라인 구성 요소

- **문서 저장소**: 검색 대상이 되는 문서들을 저장한다. 회사 내부 문서 등이 될 수 있다.
- **임베딩 모델**: 문서와 쿼리를 벡터로 변환한다. 문서는 미리 임베딩하여 저장하고, 쿼리는 검색 시점에 임베딩한다.
- **벡터 데이터베이스**: 문서 임베딩을 저장하고 유사도 검색을 수행한다.

**청킹(Chunking)**: 긴 문서를 적절한 크기로 분할한다. 너무 길면 관련 없는 내용이 포함되고, 너무 짧으면 문맥이 손실된다.

## 3. 검색 인코더 

RAG의 성능은 검색 품질에 크게 좌우된다. 검색에 사용되는 두 가지 주요 인코더 방식이 존재한다.

- **바이 인코더 (Bi-Encoder)**: 바이 인코더는 쿼리와 문서를 **독립적으로 임베딩**하는 방식이다. 각각을 별도의 벡터로 변환한 뒤, 두 벡터 간의 유사도를 계산한다.
- **교차 인코더 (Cross-Encoder)**: 교차 인코더는 쿼리와 문서를 **함께 입력**하여 유사도를 직접 예측하는 방식이다. 두 텍스트를 연결하여 하나의 입력으로 만들고, 모델이 관련성 점수를 출력한다.

각 방식의 장점을 결합하여 하이브리드 방식으로 사용할 수 있다. 

먼저 후보 추출, 바이 인코더로 전체 문서에서 후보를 빠르게 추출한다. 이 단계에서는 속도가 중요하므로 약간의 정확도 손실을 감수한다. 교차 인코더로 후보 문서들의 순위를 재조정한다. 이미 후보가 줄어든 상태이므로 정밀한 분석이 가능하다. 최종적으로 가장 관련성 높은 문서를 선택한다. 이 방식은 100만 개 문서에서 검색할 때, 100만 번이 아닌 1단계에서 추출된 후보 데이터에서 교차 인코더 연산만 수행하면 된다.
