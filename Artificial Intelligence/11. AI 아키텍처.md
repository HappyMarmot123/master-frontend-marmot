Transformer 아키텍처는 2017년 Google이 발표한 논문 "Attention Is All You Need"에서 처음 소개된 신경망 구조입니다. 이는 기존의 순환 신경망(RNN)과 장단기 기억(LSTM) 모델의 한계를 극복하고, 현재의 거대 언어 모델(LLM) 시대를 연 핵심 기술로 평가받습니다.Transformer의 핵심은 반복적인 순환 구조를 제거하고, 오직 어텐션(Attention) 메커니즘만을 사용하여 입력 시퀀스의 모든 부분을 병렬로 처리하는 데 있습니다.1. Transformer의 핵심 구성 요소: 인코더와 디코더원래의 Transformer는 주로 번역과 같은 Seq2Seq(Sequence-to-Sequence) 작업에 사용되는 인코더-디코더 구조를 가집니다.🧩 인코더 (Encoder)역할: 입력 문장(소스 시퀀스)을 받아 문장의 전체적인 맥락과 의미를 압축한 벡터 표현으로 변환합니다.특징: 각 인코더 블록은 멀티 헤드 셀프 어텐션과 피드 포워드 네트워크로 구성되며, 문장 내 모든 단어 간의 관계를 동시에 파악합니다(병렬 처리).📝 디코더 (Decoder)역할: 인코더에서 전달받은 문맥 정보를 바탕으로 타깃 언어의 문장(출력 시퀀스)을 순차적으로 생성합니다.특징: 두 가지 주요 어텐션 메커니즘을 사용합니다.마스크드 셀프 어텐션 (Masked Self-Attention): 자신이 아직 생성하지 않은 미래의 단어를 보지 못하도록 마스킹하여, 자연스러운 텍스트 생성 흐름을 유지합니다.인코더-디코더 어텐션 (Encoder-Decoder Attention): 현재 생성하려는 단어가 **입력 문장(인코더 출력)**의 어떤 부분과 가장 관련 있는지(집중해야 하는지)를 판단합니다.2. Transformer의 핵심 원리: 셀프 어텐션 (Self-Attention)셀프 어텐션은 Transformer가 혁신적인 성능을 발휘하는 핵심 메커니즘입니다. 문장 내의 각 단어가 다른 모든 단어와 얼마나 밀접한 관계를 맺고 있는지를 계산하여 문맥적 중요도를 파악합니다.Q, K, V 벡터의 역할각 입력 단어(토큰)는 학습 가능한 가중치 행렬을 통해 세 가지 벡터로 변환됩니다.쿼리 (Query, $Q$): "내가 찾고 있는 정보는 무엇인가?"를 나타내는 벡터입니다.키 (Key, $K$): "내가 가진 정보는 무엇인가?"를 나타내는 벡터로, 모든 단어가 가지고 있는 정보의 인덱스 역할을 합니다.값 (Value, $V$): "실제 전달할 정보의 내용"을 나타내는 벡터입니다.동작 원리유사도 계산: 현재 단어의 $Q$ 벡터와 문장 내 모든 단어의 $K$ 벡터를 내적하여 유사도 점수(Attention Score)를 계산합니다.가중치 정규화: 이 점수에 소프트맥스(Softmax)를 적용하여 **가중치(Weight)**로 변환합니다. 이 가중치는 해당 단어의 문맥 파악에 있어 다른 단어가 얼마나 중요한지를 나타냅니다.가중합: 계산된 가중치를 모든 단어의 $V$ 벡터에 곱하여 합산합니다. 이 최종 출력 벡터는 문장 전체의 맥락이 반영된, 해당 단어의 새로운 표현이 됩니다.3. Transformer의 주요 장점과 영향🚀 병렬 처리 능력기존 RNN이 단어를 순차적으로 처리해야 했던 것과 달리, 셀프 어텐션은 문장 내의 모든 단어 간의 관계를 한 번에(병렬로) 계산할 수 있습니다. 이는 GPU를 활용한 대규모 병렬 학습을 가능하게 하여, 모델 학습 속도를 획기적으로 향상시켰습니다.🔗 장거리 의존성 해결RNN은 문장이 길어질수록 초반 단어의 정보가 뒤로 갈수록 희미해지는 장거리 의존성(Long-range Dependency) 문제를 겪었습니다. Transformer는 셀프 어텐션을 통해 문장 내 단어 간의 거리에 상관없이 직접적인 관계를 계산할 수 있어, 긴 문맥을 정확하게 파악할 수 있게 되었습니다.🌐 현대 AI의 기반Transformer는 LLM의 시대를 연 기반 기술입니다.인코더 전용 모델: BERT (문맥 이해, 분석)디코더 전용 모델: GPT 계열 (텍스트 생성, 대화)인코더-디코더 모델: T5 (번역, 요약)이 아키텍처는 이제 자연어 처리뿐만 아니라 **비전 트랜스포머(ViT)**처럼 이미지 처리 분야에서도 활용되며 AI 전반의 표준 모델로 자리 잡았습니다.
