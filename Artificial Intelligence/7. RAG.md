# RAG (Retrieval-Augmented Generation)

RAG(Retrieval-Augmented Generation, 검색 증강 생성)는 대규모 언어 모델(LLM)의 성능을 향상시키기 위해 개발된 기술 아키텍처입니다. LLM이 답변을 생성할 때 자체적으로 학습한 지식에만 의존하지 않고, 외부의 신뢰할 수 있는 데이터에서 관련 정보를 검색(Retrieval)하여 이를 참조(Augmented)하여 답변을 생성하도록 돕습니다.

일상적인 비유로 설명하자면, RAG는 "참고서를 펼쳐놓고 시험을 보는 학생"과 같습니다. 일반적인 LLM은 학습한 내용을 기억에만 의존해서 답변하는 것과 같지만, RAG는 질문을 받으면 먼저 관련 참고서(외부 데이터베이스)를 찾아보고, 그 내용을 참고하여 정확하고 근거 있는 답변을 제공합니다. 이를 통해 최신 정보나 특정 도메인의 전문 지식을 활용할 수 있게 됩니다.

대표적인 RAG 구현 프레임워크로는 LangChain 등이 있습니다.

## 1. RAG란 무엇인가?

RAG는 두 가지 핵심 요소로 구성됩니다:

1. **검색(Retrieval)**: 사용자의 질문과 관련된 정보를 외부 데이터베이스에서 찾아오는 과정
2. **증강 생성(Augmented Generation)**: 검색된 정보를 함께 제공하여 LLM이 더 정확한 답변을 생성하도록 돕는 과정

### RAG의 핵심 아이디어

일반적인 LLM은 학습 시 제공된 데이터만을 바탕으로 답변합니다. 반면, RAG는 외부 데이터베이스에서 관련 정보를 실시간으로 검색해오고, 이를 LLM에 추가로 제공합니다. 이렇게 LLM은 자기 지식과 최신 검색 정보를 동시에 활용해 더 정확하고 신뢰할 수 있는 답변을 생성합니다.

## 2. RAG가 필요한 이유: LLM의 한계

LLM은 학습이 완료된 시점의 정적인 데이터만을 기반으로 합니다. 학습 이후의 정보나 특정 기업/도메인의 내부 문서에 대해서는 답변할 수 없거나 잘못된 정보를 제공할 수 있습니다.

**예시**:

- 2023년까지의 데이터로 학습된 LLM은 2024년의 새로운 사건을 알지 못합니다
- 특정 회사의 내부 정책 문서는 학습 데이터에 포함되지 않았을 수 있습니다
- 법률 문서나 의료 진단 가이드라인 같은 전문 도메인 정보가 부족할 수 있습니다

### 환각(Hallucination)

**문제**: LLM이 학습하지 못한 내용을 요청받았을 때, 그럴듯하지만 사실이 아닌 허위 정보를 자신 있게 생성하는 환각 현상이 발생합니다.

**예시**:

- "2025년 노벨 문학상 수상자는 누구인가?"라는 질문에, LLM은 실제 수상자를 모르기 때문에 잘못된 이름을 생성할 수 있습니다
- 특정 약물의 복용법에 대해 학습하지 못한 정보를 요청받으면, 잘못된 정보를 생성할 수 있습니다

### 투명성 부족

**문제**: 답변의 근거가 LLM 내부에 있어 사용자가 답변의 출처(Source)를 확인할 수 없습니다. 중요한 결정을 내려야 할 때 근거를 확인할 수 없다는 것은 큰 문제입니다.

**예시**:

- 의료 정보를 제공할 때 어떤 논문이나 가이드라인을 참조했는지 알 수 없습니다
- 법률 조언을 제공할 때 어떤 법률 조항을 근거로 했는지 확인할 수 없습니다

### RAG의 해결책

외부 데이터베이스에 최신 정보나 전문 문서를 저장하고, 필요할 때 검색하여 사용합니다. 외부 데이터베이스에서 실제 정보를 검색하여 제공하므로, LLM이 실제 데이터를 바탕으로 답변할 수 있습니다.
검색된 문서의 출처를 함께 제공하여 사용자가 원본 문서를 확인하고 검증할 수 있습니다.

외부 데이터베이스만 업데이트하면 되므로, 비용과 시간을 크게 절감할 수 있습니다. 또한 최신 데이터를 활용해 답변을 생성하므로 LLM의 환각 현상을 줄입니다.

## 3. RAG의 작동 원리

RAG 시스템은 주로 세 단계로 작동합니다: 임베딩/인덱싱, 검색, 생성.

### 단계 1: 임베딩/인덱싱

먼저, 외부 문서를 여러 개의 작은 조각(Chunk)으로 분할합니다. 이 과정을 청킹(Chunking)이라고 하며, LLM이 한 번에 처리할 수 있는 텍스트 길이의 한계 때문에 반드시 필요합니다. 문서를 작은 단위로 쪼개면, 더 의미 있는 관련성 검색이 가능합니다.

각 문서 조각(Chunk)은 LLM 기반 임베딩 모델로 "벡터(숫자 특징 벡터)"로 변환됩니다. 이렇게 생성된 모든 문서 조각의 벡터들은 벡터 데이터베이스(예: FAISS, Pinecone, Chroma 등)에 저장되어 효율적인 유사도 검색이 가능해집니다. 벡터 데이터베이스는 대규모 벡터 데이터를 신속하게 저장, 검색, 관리할 수 있도록 설계된 데이터베이스입니다.

### 단계 2: 검색(Retrieval)

질문(Query)이 들어오면, 질문 역시 임베딩 모델을 통해 벡터로 변환됩니다. 그 후 이 질문 벡터와 벡터 데이터베이스에 저장된 모든 문서 벡터 간의 거리를 계산하여, 가장 유사한(가까운) 상위 K개(예: 3~5개)의 문서 조각을 빠르게 찾아냅니다. 이처럼 단순한 키워드 매칭이 아니라 의미적 유사도 기반의 검색이 이루어집니다.

검색된 문서 조각들은 하나의 긴 텍스트(컨텍스트)로 합쳐지고, 이후 LLM에 전달되어 답변 생성의 기반 데이터로 사용됩니다.

### 단계 3: 생성(Generation)

마지막 단계에서는 LLM이 질문과 함께 전달받은 컨텍스트(검색된 문서 조각들)를 기반으로 답변을 생성합니다. 이 결과는 외부 데이터의 신뢰성 있는 근거나 출처와 함께 제공함으로써, 더욱 정확하고 신뢰도 높은 응답을 얻을 수 있게 됩니다.
