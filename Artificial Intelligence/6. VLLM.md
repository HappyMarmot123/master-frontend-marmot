# vLLM (Very Large Language Model Inference Engine)

vLLM은 대규모 언어 모델(LLM)의 추론(Inference) 및 서빙(Serving) 성능을 개선하기 위해 개발된 오픈소스 엔진 및 라이브러리입니다. LLM을 실제 서비스에 배포할 때 발생하는 높은 메모리 사용량과 느린 처리 속도 문제를 해결하여, 적은 비용으로도 빠른 응답 시간을 제공하고 더 많은 사용자 요청을 동시에 처리할 수 있게 합니다.

일상적인 비유로 설명하자면, vLLM은 "효율적인 주방 관리 시스템"과 같습니다. 일반적인 LLM 서빙은 마치 큰 냉장고에 음식 재료를 무작위로 넣어서 공간이 낭비되고, 요리사가 한 번에 한 개의 주문만 처리하는 것과 같습니다. vLLM은 냉장고를 작은 서랍(블록)으로 나누어 효율적으로 재료를 저장하고, 여러 주문을 동시에 처리하여 전체적인 효율을 극대화합니다.

## 1. vLLM이 필요한 이유

### LLM 서빙의 문제점

**메모리 낭비 문제**: LLM이 텍스트를 생성할 때마다 이전 토큰들의 정보를 저장해야 하는데, 각 요청마다 필요한 메모리 크기가 다릅니다. 짧은 질문과 긴 질문이 섞여 있으면 메모리가 단편화되어 사용하지 못하는 공간이 생깁니다.

**GPU 유휴 시간**: 기존 방식은 하나의 요청이 완전히 끝난 후에 다음 요청을 처리하는데, 이로 인해 GPU가 일을 하지 않는 시간이 많아집니다. 자연스럽게 높은 비용과 느린 처리 속도가 단점이 됩니다.

## 2. PagedAttention: 메모리 효율의 핵심

### 문제: KV Cache와 메모리 단편화

LLM이 텍스트를 생성할 때, 각 토큰마다 어텐션 메커니즘을 계산해야 합니다. 이를 빠르게 하기 위해 이전 토큰들의 Key-Value Cache (KV Cache)를 GPU 메모리에 저장합니다. 각 토큰의 Key와 Value 벡터를 저장한 것으로, 이를 재사용하면 계산량을 줄일 수 있습니다. 마치 책을 읽을 때 이전 페이지의 내용을 메모해두는 것과 같습니다.

**문제점**:

- 각 요청마다 필요한 KV Cache 크기가 다릅니다 (짧은 질문 vs 긴 질문)
- 메모리를 미리 할당하면 짧은 요청에 대해 공간이 낭비됩니다
- 여러 요청을 동시에 처리할 때 메모리가 단편화되어 사용하지 못하는 공간이 생깁니다

### 해결책: 운영체제의 페이징 기법 적용

PagedAttention은 컴퓨터 운영체제의 가상 메모리 및 페이징 기법에서 영감을 받았습니다. 운영체제가 물리 메모리를 작은 페이지로 나누어 효율적으로 관리하는 것처럼, vLLM도 KV Cache를 작은 블록으로 나누어 관리합니다.

**작동 방식**:

1. **블록 단위 할당**: KV Cache를 고정된 크기의 작은 블록으로 나눕니다. 각 블록은 여러 토큰의 Key-Value 쌍을 담을 수 있습니다.

2. **비연속적 저장**: 블록들을 메모리상에 비연속적으로 저장할 수 있습니다. 필요한 만큼만 블록을 할당하므로 낭비가 적습니다.

3. **블록 공유**: 여러 요청이 동일한 프롬프트 접두사(Prefix)를 사용할 경우, 해당 KV Cache 블록을 공유할 수 있습니다. 예를 들어, 시스템 프롬프트가 같으면 그 부분의 블록을 재사용합니다.

## 3. Continuous Batching: GPU 활용도 극대화

### 기존 방식의 한계: 정적 배치 처리

기존 LLM 서빙 시스템에서는 정적 배치(Static Batching) 방식이 주로 사용되었습니다. 여러 요청을 한꺼번에 모아 배치로 만든 뒤, 모든 요청이 끝날 때까지 기다려야 합니다. 이 과정에서 한 요청이 오래 걸리면, 더 빨리 끝날 수 있는 요청도 함께 지연됩니다. 그 결과 GPU는 작업이 끝나길 기다리면서 유휴(Idle) 상태가 늘어나고, 전체 처리 효율이 크게 떨어집니다.

Continuous Batching (연속적 배치)은 동적으로 배치를 관리합니다:

1. **실시간 배치 업데이트**: 새로운 요청이 들어오면 즉시 배치에 추가합니다
2. **완료된 요청 즉시 제거**: 토큰 생성이 완료된 요청은 바로 배치에서 제거합니다
3. **빈 자리 채우기**: 빠진 자리에 새로운 요청을 바로 채워넣습니다
4. **지속적 처리**: 배치가 완전히 비어있지 않는 한 계속 처리합니다
