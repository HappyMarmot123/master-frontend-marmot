자연어 처리(NLP)

NER

🔄 순환 신경망 (RNN, Recurrent Neural Network)**순환 신경망(RNN)**은 시퀀스(Sequence) 형태의 데이터, 즉 순서가 중요하고 시간적 의존성(Temporal Dependency)을 갖는 데이터를 처리하기 위해 설계된 인공 신경망의 한 종류입니다.RNN의 핵심 특징: '기억(Memory)'기존의 피드포워드 신경망(Feedforward Neural Network, MLP 등)은 각 입력 데이터를 독립적으로 처리하는 반면, RNN은 다음과 같은 독특한 구조를 가집니다.순환 구조 (Recurrent Structure): 은닉층(Hidden Layer)의 출력이 다음 시점(Time Step)의 입력으로 다시 돌아와 사용되는 루프(Loop) 구조를 가집니다.은닉 상태 (Hidden State): 이 순환 구조 덕분에, RNN은 이전 시점의 정보를 압축하여 저장하는 일종의 **내부 메모리(Hidden State)**를 갖게 됩니다. 현재의 출력을 계산할 때, 이 메모리에 저장된 과거의 맥락(Context) 정보를 함께 활용합니다.이러한 특성 덕분에 RNN은 텍스트처럼 순서에 따라 의미가 달라지는 데이터(예: "나는 사과를 먹었다"와 "나는 사과를 했다")를 처리하는 데 효과적입니다.⚠️ RNN의 한계와 발전기본적인 RNN은 긴 시퀀스를 처리할 때 **장기 의존성(Long-Term Dependency)**을 학습하는 데 어려움을 겪는 기울기 소실(Vanishing Gradient) 문제에 취약했습니다.이 문제를 해결하기 위해, 메모리 구조를 개선한 고급 RNN 아키텍처들이 등장했습니다.LSTM (Long Short-Term Memory): '셀 상태(Cell State)'와 '게이트(Gate)' 구조를 추가하여 긴 시퀀스에서도 중요한 정보를 효과적으로 기억하고 불필요한 정보를 잊을 수 있게 설계되었습니다.GRU (Gated Recurrent Unit): LSTM보다 게이트 수가 적어 계산량이 적으면서도 유사한 성능을 내도록 단순화된 모델입니다.🔗 NLP와의 연관성네, 순환 신경망(RNN)은 자연어 처리(NLP) 분야에서 가장 핵심적인 모델 중 하나였습니다.언어(텍스트)는 본질적으로 순차 데이터이기 때문에, 순서를 고려하여 이전 단어가 다음 단어의 의미에 미치는 영향을 파악해야 합니다. RNN은 이러한 언어의 특성을 모델링하는 데 가장 적합했습니다.NLP에서의 주요 활용 사례NLP 작업RNN의 역할언어 모델링이전 단어들을 바탕으로 다음에 올 단어를 예측합니다. (예: 스마트폰 키보드의 자동 완성)기계 번역인코더-디코더(Encoder-Decoder) 구조의 핵심으로 사용되어, 입력 문장(소스)을 이해(인코딩)하고 목표 언어의 문장(타겟)을 생성(디코딩)합니다.텍스트 생성주어진 시작 텍스트를 기반으로 이야기를 이어 나가거나 코드를 생성합니다.감성 분석문장의 앞부분 단어들이 뒷부분 단어에 미치는 영향을 종합적으로 파악하여 전체적인 감정을 판단합니다.현재의 위치RNN, 특히 LSTM과 GRU는 오랫동안 NLP 분야의 표준 모델로 사용되었지만, 최근에는 트랜스포머(Transformer) 아키텍처(BERT, GPT 등)가 병렬 처리의 이점과 **어텐션 메커니즘(Attention Mechanism)**을 통해 장기 의존성 문제를 더 효과적으로 해결하며 NLP 분야의 주류를 이루고 있습니다.



NLP 전처리 후 표현 방식 현대적(Word2Vec, BERT)
