# LLM (Large Language Model)

LLM(Large Language Model, 대규모 언어 모델)은 많은 매개변수를 가진 인공지능 모델로, 인간의 언어를 이해하고 생성할 수 있는 능력을 갖춘 현대 AI의 핵심 기술입니다. ChatGPT, GPT-4, Claude, Gemini 등 우리가 일상에서 사용하는 대부분의 AI 챗봇과 언어 서비스의 기반이 되는 기술입니다.

LLM을 이해하는 가장 좋은 비유는 "도서관"입니다. 이 모델은 인터넷상의 수많은 책, 논문, 기사, 코드 등을 학습하여, 마치 인간이 수년간 책을 읽으며 언어를 습득하듯이, 단어와 문장의 패턴, 맥락, 의미를 이해하게 됩니다. 그리고 이러한 지식을 바탕으로 질문에 답하고, 글을 쓰고, 코드를 생성하며, 창의적인 작업을 수행할 수 있습니다.

## 1. LLM이란 무엇인가?

### 언어 모델의 기본 개념

언어 모델(Language Model)은 주어진 단어 시퀀스에서 다음 단어가 무엇일지 예측하는 확률 모델입니다. 예를 들어, "오늘 날씨가 매우"라는 문장이 주어졌을 때, 모델은 "맑다", "흐리다", "따뜻하다" 등의 단어 중에서 어떤 단어가 가장 적절한지 확률을 계산하여 예측합니다.

간단한 예시를 들어보겠습니다. "나는 매일 아침에"라는 문장을 완성한다고 가정해봅시다. 인간이라면 자연스럽게 "커피를 마신다", "운동을 한다", "신문을 읽는다" 등의 문장을 생각할 수 있습니다. 언어 모델도 마찬가지로, 학습한 수많은 문장 패턴을 바탕으로 다음 단어를 예측합니다.

### LLM의 'Large'는 무엇을 의미하는가?

LLM의 "Large"는 모델의 크기를 의미합니다. 모델이 클수록 더 복잡한 패턴을 학습할 수 있고, 더 정확하고 맥락에 맞는 언어를 생성할 수 있습니다. 하지만 동시에 더 많은 계산 자원과 비용이 필요합니다.

## 2. LLM의 작동 원리: 토큰화와 예측

### 토큰화 (Tokenization)

LLM이 텍스트를 처리하는 첫 번째 단계는 토큰화(Tokenization)입니다. 토큰화는 텍스트를 작은 단위로 나누는 과정입니다. 이 단위는 단어일 수도 있고, 단어의 일부일 수도 있으며, 문자일 수도 있습니다.

예를 들어, "안녕하세요"라는 텍스트를 토큰화하면:

- 단어 단위: ["안녕하세요"]
- 문자 단위: ["안", "녕", "하", "세", "요"]
- 서브워드 단위: ["안녕", "하세요"]

실제 LLM에서는 주로 서브워드(Subword) 토큰화를 사용합니다. 이렇게 하면 학습하지 못한 단어(Out-of-Vocabulary)를 만났을 때도, 작은 토큰들의 조합으로 처리할 수 있기 때문입니다.

### 임베딩 (Embedding)

토큰화된 단어들은 숫자 벡터로 변환됩니다. 이 과정을 임베딩(Embedding)이라고 합니다. 각 단어는 수백 차원의 벡터로 표현되며, 이 벡터는 단어의 의미와 문맥을 담고 있습니다.

임베딩의 마법은 유사한 의미를 가진 단어들이 공간상에서 가까운 위치에 배치된다는 것입니다. 예를 들어, "고양이"와 "강아지"의 임베딩 벡터는 "컴퓨터"의 임베딩 벡터보다 서로 가까이 있을 것입니다.

1. **입력 처리**: 사용자가 입력한 텍스트를 토큰화하고 임베딩 벡터로 변환
2. **맥락 이해**: 트랜스포머 아키텍처를 통해 모든 토큰 간의 관계와 맥락을 이해
3. **확률 계산**: 다음에 올 수 있는 모든 가능한 토큰에 대한 확률을 계산
4. **토큰 선택**: 확률이 가장 높은 토큰을 선택하거나, 확률 분포에 따라 샘플링
5. **반복**: 선택된 토큰을 문장에 추가하고, 다시 다음 토큰을 예측하는 과정을 반복

### 어텐션 메커니즘 (Attention Mechanism)

LLM이 문맥을 이해하는 핵심 메커니즘은 어텐션(Attention)입니다. 어텐션은 문장의 각 단어가 다른 단어들과 얼마나 관련이 있는지를 계산하여, 중요한 정보에 더 집중할 수 있게 해줍니다.

예를 들어, "그는 학교에 가서 친구를 만났다"라는 문장에서, "만났다"라는 동사를 이해하려면 "그는"과 "친구를"에 주목해야 합니다. 어텐션 메커니즘은 이러한 관계를 자동으로 학습하여, 문맥을 올바르게 이해할 수 있게 합니다.

## 3. LLM의 학습 과정

### 사전 학습 (Pre-training)

LLM의 학습은 두 단계로 나뉩니다. 첫 번째 단계는 사전 학습(Pre-training)입니다. 이 단계에서 모델은 인터넷상의 방대한 텍스트 데이터를 학습합니다. 목표는 언어의 일반적인 패턴, 문법, 사실 관계, 맥락 등을 이해하는 것입니다.

사전 학습은 주로 언어 모델링(Language Modeling) 작업을 통해 이루어집니다. 모델에게 문장의 일부를 보여주고, 다음 단어를 예측하도록 학습시킵니다. 이 과정을 수조 개의 문장에 대해 반복하면서, 모델은 점차 언어의 패턴을 이해하게 됩니다.

예를 들어, "파리는 프랑스의 수도이다"라는 문장이 주어졌을 때, 모델은 "파리는" 다음에 "프랑스의"가 오는 패턴을 학습하고, 동시에 "파리"와 "프랑스"가 관련이 있다는 사실도 암묵적으로 학습합니다.

## 4. 프롬프트 엔지니어링 (Prompt Engineering)

LLM의 성능은 프롬프트의 질에 크게 좌우됩니다. 같은 모델이라도 잘 작성된 프롬프트는 훨씬 좋은 결과를 만들어내는 반면, 불명확한 프롬프트는 만족스럽지 못한 결과를 만들 수 있습니다.

예를 들어, 단순히 "요약해줘"라고 하기보다는, "다음 텍스트를 3문장으로 요약해줘. 핵심 내용만 포함하고 간결하게 작성해줘"라고 하면 훨씬 원하는 결과를 얻을 수 있습니다.

### 효과적인 프롬프트 작성법

**1. 명확한 지시**: 모호한 표현보다 구체적인 지시를 사용합니다.

- 나쁜 예: "이것에 대해 설명해줘"
- 좋은 예: "이 코드 블록의 기능을 3가지로 나누어 설명하고, 각각의 역할을 설명해줘"

**2. 컨텍스트 제공**: 필요한 배경 정보를 포함합니다.

- 나쁜 예: "번역해줘"
- 좋은 예: "다음 영어 문장을 자연스러운 한국어로 번역해줘. 비즈니스 문서의 일부입니다."

**3. 출력 형식 지정**: 원하는 출력 형식을 명시합니다.

- 나쁜 예: "리스트를 만들어줘"
- 좋은 예: "다음 항목들을 마크다운 형식의 불릿 리스트로 정리해줘"

**4. 예시 제공 (Few-shot Learning)**: 원하는 출력 예시를 함께 제공하면 더 정확한 결과를 얻을 수 있습니다.

**5. 단계별 접근**: 복잡한 작업은 여러 단계로 나누어 요청합니다.
