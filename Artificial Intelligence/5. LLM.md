# LLM (Large Language Model)

LLM(Large Language Model, 대규모 언어 모델)은 많은 매개변수를 가진 인공지능 모델로, 인간의 언어를 이해하고 생성할 수 있는 능력을 갖춘 현대 AI의 핵심 기술입니다. ChatGPT, GPT-4, Claude, Gemini 등 우리가 일상에서 사용하는 대부분의 AI 챗봇과 언어 서비스의 기반이 되는 기술입니다.

LLM을 이해하는 가장 좋은 비유는 "도서관"입니다. 이 모델은 인터넷상의 수많은 책, 논문, 기사, 코드 등을 학습하여, 마치 인간이 수년간 책을 읽으며 언어를 습득하듯이, 단어와 문장의 패턴, 맥락, 의미를 이해하게 됩니다. 그리고 이러한 지식을 바탕으로 질문에 답하고, 글을 쓰고, 코드를 생성하며, 창의적인 작업을 수행할 수 있습니다.

## 1. LLM이란 무엇인가?

### 언어 모델의 기본 개념

언어 모델(Language Model)은 주어진 단어 시퀀스에서 다음 단어가 무엇일지 예측하는 확률 모델입니다. 예를 들어, "오늘 날씨가 매우"라는 문장이 주어졌을 때, 모델은 "맑다", "흐리다", "따뜻하다" 등의 단어 중에서 어떤 단어가 가장 적절한지 확률을 계산하여 예측합니다.

간단한 예시를 들어보겠습니다. "나는 매일 아침에"라는 문장을 완성한다고 가정해봅시다. 인간이라면 자연스럽게 "커피를 마신다", "운동을 한다", "신문을 읽는다" 등의 문장을 생각할 수 있습니다. 언어 모델도 마찬가지로, 학습한 수많은 문장 패턴을 바탕으로 다음 단어를 예측합니다.

## 2. LLM의 작동 원리: 토큰화와 예측

### 토큰화 (Tokenization)

LLM이 텍스트를 처리하는 첫 번째 단계는 토큰화(Tokenization)입니다. 토큰화는 텍스트를 작은 단위로 나누는 과정입니다. 이 단위는 단어일 수도 있고, 단어의 일부일 수도 있으며, 문자일 수도 있습니다.

예를 들어, "안녕하세요"라는 텍스트를 토큰화하면:

- 단어 단위: ["안녕하세요"]
- 문자 단위: ["안", "녕", "하", "세", "요"]
- 서브워드 단위: ["안녕", "하세요"]

실제 LLM에서는 주로 서브워드(Subword) 토큰화를 사용합니다. 이렇게 하면 학습하지 못한 단어(Out-of-Vocabulary)를 만났을 때도, 작은 토큰들의 조합으로 처리할 수 있기 때문입니다.

### 어텐션 메커니즘 (Attention Mechanism)

LLM이 문맥을 이해하는 핵심 메커니즘은 어텐션(Attention)입니다. 어텐션은 문장의 각 단어가 다른 단어들과 얼마나 관련이 있는지를 계산하여, 중요한 정보에 더 집중할 수 있게 해줍니다.

예를 들어, "그는 학교에 가서 친구를 만났다"라는 문장에서, "만났다"라는 동사를 이해하려면 "그는"과 "친구를"에 주목해야 합니다. 어텐션 메커니즘은 이러한 관계를 자동으로 학습하여, 문맥을 올바르게 이해할 수 있게 합니다.

## 3. LLM의 학습 과정

### 사전 학습 (Pre-training)

LLM의 학습은 두 단계로 나뉩니다. 첫 번째 단계는 사전 학습(Pre-training)입니다. 이 단계에서 모델은 인터넷상의 방대한 텍스트 데이터를 학습합니다. 목표는 언어의 일반적인 패턴, 문법, 사실 관계, 맥락 등을 이해하는 것입니다.

사전 학습은 주로 언어 모델링(Language Modeling) 작업을 통해 이루어집니다. 모델에게 문장의 일부를 보여주고, 다음 단어를 예측하도록 학습시킵니다. 이 과정을 수조 개의 문장에 대해 반복하면서, 모델은 점차 언어의 패턴을 이해하게 됩니다.

예를 들어, "파리는 프랑스의 수도이다"라는 문장이 주어졌을 때, 모델은 "파리는" 다음에 "프랑스의"가 오는 패턴을 학습하고, 동시에 "파리"와 "프랑스"가 관련이 있다는 사실도 암묵적으로 학습합니다.

## 퀀타이제이션 (Quantization)

퀀타이제이션, 즉 양자화는 연속적인 값의 범위(예: 실수)를 제한된 수의 이산적인 값으로 변환하는 과정을 의미합니다. AI 분야에서 퀀타이제이션은 딥러닝 모델의 경량화(Model Compression) 및 배포 효율화를 위한 필수 기술로 사용됩니다.

낮은 정밀도의 정수 형식으로 변환하는 과정입니다. 정수 연산은 부동 소수점 연산보다 하드웨어에서 훨씬 빠르고 효율적으로 처리됩니다. 모델 크기를 1/4 (FP32 → INT8) 등으로 대폭 줄여 메모리 요구량을 낮춥니다. 이는 특히 대규모 언어 모델(LLM)을 작은 디바이스나 제한된 서버 자원에서 구동할 때 결정적인 역할을 합니다.
