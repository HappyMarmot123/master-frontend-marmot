## Transformer (트랜스포머)

2017년 Google에서 발표한 딥러닝 아키텍처로, 핵심은 어텐션 메커니즘(Attention Mechanism)에 있습니다. Transformer는 기존의 RNN과 달리 입력 문장의 모든 단어들 간의 관계를 한 번에 계산할 수 있어, 병렬 처리가 매우 용이합니다.

### Transformer의 핵심 구성 요소

#### 1. 어텐션 메커니즘 (Attention Mechanism)

어텐션 메커니즘이란, 입력 시퀀스 내의 각 요소(예: 단어)가 다른 모든 요소와 어느 정도 관련이 있는지, 즉 어느 부분에 "집중"해야 하는지를 자동으로 계산하는 방법입니다. 쉽게 말해, 한 단어를 이해할 때 전체 문장의 어떤 단어들과 중요한 관계를 맺는지 알아내어 그 정보를 더 잘 반영하는 방식입니다. Transformer는 이 어텐션 메커니즘만을 사용해 시퀀스 데이터를 처리합니다.

**예시**: "고양이는 작은 동물이다"라는 문장에서:

- "고양이"를 이해할 때 "동물"과 높은 관련성을 가집니다
- "작은"은 "고양이"와 "동물" 모두와 관련이 있습니다
- "이다"는 "고양이"와 높은 관련성을 가집니다

#### 2. 셀프 어텐션 (Self-Attention)

셀프 어텐션은 같은 문장 내의 단어들이 서로 어떻게 관련되는지를 계산합니다. 문장의 모든 단어를 동시에 고려하여 각 단어의 표현을 개선합니다.

**작동 방식**:

1. 각 단어를 Query(질문), Key(키), Value(값) 세 가지 벡터로 변환
2. Query와 Key의 유사도를 계산하여 어텐션 점수 생성
3. 어텐션 점수를 사용하여 Value들의 가중 평균 계산
4. 결과적으로 각 단어가 문맥을 고려한 새로운 표현을 얻음

#### 3. 포지셔널 인코딩 (Positional Encoding)

Transformer는 모든 단어를 동시에 처리하므로, 단어의 순서 정보를 잃을 수 있습니다. 포지셔널 인코딩은 각 단어의 위치 정보를 추가하여, 모델이 단어의 순서를 이해할 수 있게 합니다.

마치 책의 페이지 번호를 매기는 것과 같습니다. 모든 페이지를 동시에 펼쳐놓아도, 페이지 번호가 있으면 순서를 알 수 있습니다.

### Transformer의 작동 과정

1. **입력 임베딩**: 각 단어를 벡터로 변환하고 위치 정보를 추가
2. **인코더 처리**: 여러 층의 인코더를 통해 문맥을 이해
3. **디코더 처리**: 인코더의 출력을 바탕으로 새 문장 생성
4. **출력 생성**: 최종적으로 번역된 문장이나 생성된 텍스트 출력

## BLEU Score (BLEU 스코어)

BLEU(Bilingual Evaluation Understudy)는 기계 번역 시스템의 품질을 평가하는 지표입니다. 번역된 문장이 사람이 번역한 참조 번역(Reference Translation)과 얼마나 유사한지를 측정합니다.

일상적인 비유로 설명하자면, BLEU Score는 "번역 시험의 채점"과 같습니다. 학생이 번역한 답안(기계 번역)과 정답(참조 번역)을 비교하여, 얼마나 비슷한지 점수를 매깁니다. 단어가 일치하는 정도뿐만 아니라, 단어의 순서와 조합도 고려합니다.

기계 번역 시스템을 개발할 때, 번역의 품질을 객관적으로 평가해야 합니다. 사람이 직접 평가하는 것은 시간이 오래 걸리고 주관적일 수 있습니다. BLEU Score는 자동으로 객관적인 평가를 제공합니다.
